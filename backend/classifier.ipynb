{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbb46b0",
   "metadata": {},
   "source": [
    "# Document Classification with BART-Large-MNLI\n",
    "\n",
    "This notebook explores document classification using Facebook's BART-Large-MNLI model for our Smart Document Classifier API.\n",
    "\n",
    "We'll use the **pipeline approach** (Option 1) as it's more suitable for our FastAPI integration:\n",
    "- Simpler implementation\n",
    "- Better error handling\n",
    "- Built-in optimizations\n",
    "- Easier to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28923d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ccf75",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f8374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART-Large-MNLI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the zero-shot classification pipeline with BART-Large-MNLI\n",
    "print(\"Loading BART-Large-MNLI model...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbc92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document categories: ['Technical Documentation', 'Business Proposal', 'Legal Document', 'Academic Paper', 'General Article', 'Other']\n",
      "Total categories: 6\n"
     ]
    }
   ],
   "source": [
    "# Define document categories for classification\n",
    "DOCUMENT_CATEGORIES = [\n",
    "    \"Technical Documentation\",\n",
    "    \"Business Proposal\", \n",
    "    \"Legal Document\",\n",
    "    \"Academic Paper\",\n",
    "    \"General Article\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "print(f\"Document categories: {DOCUMENT_CATEGORIES}\")\n",
    "print(f\"Total categories: {len(DOCUMENT_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document classification function created!\n"
     ]
    }
   ],
   "source": [
    "def classify_document(text: str, categories: List[str] = DOCUMENT_CATEGORIES) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classify a document using BART-Large-MNLI zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        categories: List of possible categories\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # IMPROVED: Use tokenizer-based truncation instead of character truncation\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "        \n",
    "        # Count actual tokens, not characters\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        max_tokens = 800  # Conservative limit for BART\n",
    "        \n",
    "        if len(tokens) > max_tokens:\n",
    "            # Proper token-based truncation\n",
    "            truncated_tokens = tokens[:max_tokens]\n",
    "            text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "            print(f\"⚠️  Text truncated from {len(tokens)} to {max_tokens} tokens\")\n",
    "        \n",
    "        # Perform classification\n",
    "        start_time = time.time()\n",
    "        result = classifier(text, categories)\n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"📊 Classification result: {result['labels'][0]} ({result['scores'][0]:.4f})\")\n",
    "        \n",
    "        # Format results\n",
    "        classification_result = {\n",
    "            \"predicted_category\": result[\"labels\"][0],\n",
    "            \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "            \"all_scores\": {\n",
    "                label: round(score, 4) \n",
    "                for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "            },\n",
    "            \"inference_time\": round(inference_time, 3),\n",
    "            \"model_used\": \"facebook/bart-large-mnli\",\n",
    "            \"token_count\": len(tokens),\n",
    "            \"was_truncated\": len(tokens) > max_tokens\n",
    "        }\n",
    "        \n",
    "        return classification_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"predicted_category\": None,\n",
    "            \"confidence_score\": 0.0\n",
    "        }\n",
    "\n",
    "print(\"✅ Updated document classification function with proper tokenizer-based truncation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old vs new truncation methods\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for proper token counting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "test_doc = \"\"\"\n",
    "LEGAL AGREEMENT - SOFTWARE LICENSE TERMS\n",
    "\n",
    "IMPORTANT: Please read these terms and conditions carefully before using our software.\n",
    "\n",
    "1. GRANT OF LICENSE\n",
    "Subject to the terms and conditions of this Agreement, Company hereby grants to you a limited, non-exclusive, non-transferable license to use the software solely for your internal business purposes.\n",
    "\n",
    "2. RESTRICTIONS\n",
    "You may not: (a) modify, adapt, or create derivative works; (b) reverse engineer, decompile, or disassemble; (c) remove or alter any proprietary notices; (d) distribute, sublicense, or transfer the software.\n",
    "\n",
    "3. INTELLECTUAL PROPERTY\n",
    "All intellectual property rights in and to the software remain the exclusive property of Company. No rights are granted except as expressly set forth herein.\n",
    "\n",
    "4. WARRANTY DISCLAIMER\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.\n",
    "\n",
    "5. LIMITATION OF LIABILITY\n",
    "IN NO EVENT SHALL COMPANY BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING OUT OF OR RELATING TO THIS AGREEMENT.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 TRUNCATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original document: {len(test_doc)} characters, {len(tokenizer.encode(test_doc))} tokens\")\n",
    "print()\n",
    "\n",
    "# OLD METHOD (Character-based - WRONG)\n",
    "old_truncated = test_doc[:300] + \"...\"\n",
    "print(\"❌ OLD METHOD (Character truncation at 300 chars):\")\n",
    "print(f\"Result: {len(old_truncated)} chars, {len(tokenizer.encode(old_truncated))} tokens\")\n",
    "print(f\"Preview: {repr(old_truncated[:100])}...\")\n",
    "print()\n",
    "\n",
    "# NEW METHOD (Token-based - CORRECT)\n",
    "tokens = tokenizer.encode(test_doc, add_special_tokens=False)\n",
    "max_tokens = 80  # Small limit for demo\n",
    "if len(tokens) > max_tokens:\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    new_truncated = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "else:\n",
    "    new_truncated = test_doc\n",
    "\n",
    "print(\"✅ NEW METHOD (Token truncation at 80 tokens):\")\n",
    "print(f\"Result: {len(new_truncated)} chars, {len(tokenizer.encode(new_truncated))} tokens\")\n",
    "print(f\"Preview: {repr(new_truncated[:100])}...\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 KEY DIFFERENCES:\")\n",
    "print(\"1. Old method cuts mid-sentence, new method respects token boundaries\")\n",
    "print(\"2. Old method doesn't account for tokenization differences\")\n",
    "print(\"3. New method ensures exact token count for BART model\")\n",
    "print(\"4. New method preserves more meaningful content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b50cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing document classification...\n",
      "{'sequence': '\\nThis document outlines the technical specifications for implementing a RESTful API \\nusing FastAPI framework. The API includes endpoints for document upload, processing, \\nand classification. Key components include SQLAlchemy for database operations, \\nPydantic for data validation, and uvicorn as the ASGI server.\\n', 'labels': ['Technical Documentation', 'General Article', 'Other', 'Academic Paper', 'Business Proposal', 'Legal Document'], 'scores': [0.8104618191719055, 0.07211217284202576, 0.06284535676240921, 0.022135350853204727, 0.018267560750246048, 0.014177760109305382]}\n",
      "\n",
      "Classification Result:\n"
     ]
    }
   ],
   "source": [
    "# Test the classification with a sample document\n",
    "sample_text = \"\"\"\n",
    "This document outlines the technical specifications for implementing a RESTful API \n",
    "using FastAPI framework. The API includes endpoints for document upload, processing, \n",
    "and classification. Key components include SQLAlchemy for database operations, \n",
    "Pydantic for data validation, and uvicorn as the ASGI server.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing document classification...\")\n",
    "result = classify_document(sample_text)\n",
    "# print(\"\\nClassification Result:\")\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dac667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some documents from our dataset\n",
    "dataset_path = \"../ml/Dataset/\"\n",
    "\n",
    "# Read a few sample documents\n",
    "sample_files = [\n",
    "    \"python_doc.txt\",\n",
    "    \"How I use LLMs as a staff engineer.txt\", \n",
    "    \"compujai.txt\"\n",
    "]\n",
    "\n",
    "print(\"Testing classification on existing dataset documents:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename in sample_files:\n",
    "    filepath = os.path.join(dataset_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"\\nFile: {filename}\")\n",
    "        print(f\"Content preview: {content[:100]}...\")\n",
    "        \n",
    "        result = classify_document(content)\n",
    "        print(f\"Predicted Category: {result['predicted_category']}\")\n",
    "        print(f\"Confidence: {result['confidence_score']}\")\n",
    "        print(f\"Inference Time: {result['inference_time']}s\")\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed16b",
   "metadata": {},
   "source": [
    "## Creating ML Module for FastAPI Integration\n",
    "\n",
    "Now let's create the classifier module that will be integrated into our FastAPI backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cbcdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create the ML classifier module for FastAPI integration\n",
    "classifier_module_code = '''\n",
    "\"\"\"\n",
    "Document Classifier Module using BART-Large-MNLI\n",
    "For Smart Document Classifier FastAPI Application\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentClassifier:\n",
    "    \"\"\"Document classifier using Facebook's BART-Large-MNLI model\"\"\"\n",
    "    \n",
    "    CATEGORIES = [\n",
    "        \"Technical Documentation\",\n",
    "        \"Business Proposal\", \n",
    "        \"Legal Document\",\n",
    "        \"Academic Paper\",\n",
    "        \"General Article\",\n",
    "        \"Other\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier\"\"\"\n",
    "        self.classifier = None\n",
    "        self.model_name = \"facebook/bart-large-mnli\"\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the BART-Large-MNLI model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading {self.model_name} model...\")\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\", \n",
    "                model=self.model_name,\n",
    "                device=-1  # Use CPU, change to 0 for GPU\n",
    "            )\n",
    "            self.is_loaded = True\n",
    "            logger.info(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def classify(self, text: str, categories: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify a document\n",
    "        \n",
    "        Args:\n",
    "            text: Document text to classify\n",
    "            categories: Optional custom categories (defaults to self.CATEGORIES)\n",
    "            \n",
    "        Returns:\n",
    "            Classification results with confidence scores\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "            \n",
    "        if not text or not text.strip():\n",
    "            return {\n",
    "                \"error\": \"Empty text provided\",\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "            \n",
    "        categories = categories or self.CATEGORIES\n",
    "        \n",
    "        try:\n",
    "            # Truncate text if too long (BART token limit ~1024)\n",
    "            max_length = 800  # Conservative limit for better performance\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "                logger.info(f\"Text truncated to {max_length} characters\")\n",
    "            \n",
    "            # Perform classification\n",
    "            start_time = time.time()\n",
    "            result = self.classifier(text, categories)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Format results\n",
    "            classification_result = {\n",
    "                \"predicted_category\": result[\"labels\"][0],\n",
    "                \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "                \"all_scores\": {\n",
    "                    label: round(score, 4) \n",
    "                    for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "                },\n",
    "                \"inference_time\": round(inference_time, 3),\n",
    "                \"model_used\": self.model_name,\n",
    "                \"text_length\": len(text)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Classification completed: {result['labels'][0]} ({result['scores'][0]:.4f})\")\n",
    "            return classification_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "\n",
    "# Global classifier instance (singleton pattern)\n",
    "_classifier_instance = None\n",
    "\n",
    "def get_classifier() -> DocumentClassifier:\n",
    "    \"\"\"Get or create the global classifier instance\"\"\"\n",
    "    global _classifier_instance\n",
    "    if _classifier_instance is None:\n",
    "        _classifier_instance = DocumentClassifier()\n",
    "    return _classifier_instance\n",
    "\n",
    "def classify_document_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience function to classify document text\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        \n",
    "    Returns:\n",
    "        Classification results\n",
    "    \"\"\"\n",
    "    classifier = get_classifier()\n",
    "    return classifier.classify(text)\n",
    "'''\n",
    "\n",
    "# Write the module to a file\n",
    "with open('../backend/ml_classifier.py', 'w') as f:\n",
    "    f.write(classifier_module_code)\n",
    "\n",
    "print(\"✅ ML classifier module created at: backend/ml_classifier.py\")\n",
    "print(\"📦 Module includes:\")\n",
    "print(\"   - DocumentClassifier class\")\n",
    "print(\"   - Singleton pattern for model loading\")\n",
    "print(\"   - Error handling and logging\")\n",
    "print(\"   - Performance optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc8621",
   "metadata": {},
   "source": [
    "## Fixing Multiprocessing Resource Warning\n",
    "\n",
    "The warning about leaked semaphore objects occurs because the transformers library uses multiprocessing resources that aren't properly cleaned up on shutdown. We've implemented proper resource management:\n",
    "\n",
    "1. **Added cleanup methods** to the DocumentClassifier class\n",
    "2. **Registered atexit handlers** to cleanup on process termination  \n",
    "3. **Added FastAPI shutdown event** to cleanup ML resources\n",
    "4. **Explicit resource management** with garbage collection and PyTorch cache clearing\n",
    "\n",
    "This prevents the resource tracker warning: `resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a6970",
   "metadata": {},
   "source": [
    "## Better Text Truncation Strategies\n",
    "\n",
    "The current simple truncation `text[:1000]` is problematic because:\n",
    "\n",
    "1. **Character vs Token mismatch** - BART uses BPE tokenization, not character counting\n",
    "2. **Loses important context** - May cut mid-sentence or lose document structure  \n",
    "3. **Suboptimal for classification** - Beginning might not contain key classification signals\n",
    "4. **No intelligent splitting** - Doesn't respect word/sentence boundaries\n",
    "\n",
    "Let's implement better strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load BART tokenizer to properly count tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "def smart_truncate_text(text: str, max_tokens: int = 800) -> str:\n",
    "    \"\"\"\n",
    "    Intelligently truncate text for BART classification\n",
    "    \n",
    "    Strategies:\n",
    "    1. Use actual tokenizer to count tokens, not characters\n",
    "    2. Take beginning + end of document (sandwich approach)  \n",
    "    3. Preserve sentence boundaries\n",
    "    4. Include document structure clues (titles, headers)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: Simple tokenizer-based truncation\n",
    "    def tokenizer_truncate(text: str, max_tokens: int) -> str:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "        \n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        return tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Strategy 2: Sandwich approach (beginning + end)\n",
    "    def sandwich_truncate(text: str, max_tokens: int) -> str:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "            \n",
    "        # Take 60% from beginning, 40% from end\n",
    "        start_tokens = int(max_tokens * 0.6)\n",
    "        end_tokens = max_tokens - start_tokens\n",
    "        \n",
    "        beginning = tokenizer.decode(tokens[:start_tokens], skip_special_tokens=True)\n",
    "        ending = tokenizer.decode(tokens[-end_tokens:], skip_special_tokens=True)\n",
    "        \n",
    "        return f\"{beginning}\\n\\n[...document continues...]\\n\\n{ending}\"\n",
    "    \n",
    "    # Strategy 3: Smart chunking (preserve sentences)\n",
    "    def smart_chunk_truncate(text: str, max_tokens: int) -> str:\n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        result_tokens = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            \n",
    "            if current_length + len(sentence_tokens) <= max_tokens:\n",
    "                result_tokens.extend(sentence_tokens)\n",
    "                current_length += len(sentence_tokens)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if result_tokens:\n",
    "            return tokenizer.decode(result_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            # Fallback to simple truncation if first sentence is too long\n",
    "            return tokenizer_truncate(text, max_tokens)\n",
    "    \n",
    "    # Choose strategy based on document characteristics\n",
    "    token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "    \n",
    "    if token_count <= max_tokens:\n",
    "        return text\n",
    "    elif token_count > max_tokens * 3:  # Very long document\n",
    "        return sandwich_truncate(text, max_tokens)\n",
    "    else:  # Moderately long document\n",
    "        return smart_chunk_truncate(text, max_tokens)\n",
    "\n",
    "# Test the different approaches\n",
    "test_text = \"\"\"\n",
    "# Technical Documentation: FastAPI Implementation Guide\n",
    "\n",
    "This comprehensive guide covers the implementation of a FastAPI web application for document classification.\n",
    "\n",
    "## Architecture Overview\n",
    "The system uses a modular architecture with the following components:\n",
    "- FastAPI framework for REST API endpoints\n",
    "- SQLAlchemy ORM for database operations  \n",
    "- Pydantic models for data validation\n",
    "- BART-Large-MNLI for document classification\n",
    "- Uvicorn ASGI server for deployment\n",
    "\n",
    "## Implementation Details\n",
    "The core application consists of several modules that work together to provide document classification capabilities.\n",
    "\n",
    "### Database Layer\n",
    "The database layer uses SQLAlchemy to manage document metadata and classification results.\n",
    "\n",
    "### ML Classification\n",
    "The machine learning component uses Facebook's BART-Large-MNLI model for zero-shot classification.\n",
    "\n",
    "## Performance Considerations\n",
    "For production deployment, consider the following optimizations:\n",
    "- Model caching and singleton patterns\n",
    "- Async processing for better throughput  \n",
    "- Resource cleanup to prevent memory leaks\n",
    "- Proper error handling and logging\n",
    "\n",
    "## Conclusion\n",
    "This FastAPI implementation provides a robust foundation for document classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text length:\", len(test_text), \"characters\")\n",
    "print(\"Original token count:\", len(tokenizer.encode(test_text, add_special_tokens=False)), \"tokens\")\n",
    "print()\n",
    "\n",
    "# Test different truncation methods\n",
    "truncated = smart_truncate_text(test_text, max_tokens=100)\n",
    "print(\"Smart truncated length:\", len(tokenizer.encode(truncated, add_special_tokens=False)), \"tokens\")\n",
    "print(\"Smart truncated text:\")\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd927069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problem with simple character truncation\n",
    "sample_document = \"\"\"\n",
    "LEGAL AGREEMENT - TERMS OF SERVICE\n",
    "\n",
    "IMPORTANT: Please read these terms carefully before using our service.\n",
    "\n",
    "1. ACCEPTANCE OF TERMS\n",
    "By accessing and using this service, you agree to be bound by the terms and conditions outlined in this agreement.\n",
    "\n",
    "2. SERVICE DESCRIPTION  \n",
    "Our document classification service uses artificial intelligence to automatically categorize uploaded documents into predefined categories.\n",
    "\n",
    "3. USER OBLIGATIONS\n",
    "Users must ensure that uploaded documents do not contain:\n",
    "- Confidential or proprietary information\n",
    "- Personal identifying information (PII) \n",
    "- Copyrighted material without permission\n",
    "- Malicious code or harmful content\n",
    "\n",
    "4. LIMITATION OF LIABILITY\n",
    "In no event shall the company be liable for any indirect, incidental, special, consequential, or punitive damages.\n",
    "\n",
    "5. TERMINATION\n",
    "We reserve the right to terminate or suspend access to our service immediately, without prior notice.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== PROBLEM WITH SIMPLE CHARACTER TRUNCATION ===\")\n",
    "print()\n",
    "\n",
    "# Current problematic approach (character-based)\n",
    "simple_truncated = sample_document[:200] + \"...\"\n",
    "print(\"Simple character truncation (200 chars):\")\n",
    "print(repr(simple_truncated))\n",
    "print()\n",
    "\n",
    "# Show what happens with tokenization\n",
    "print(\"Tokens in simple truncated text:\", len(tokenizer.encode(simple_truncated, add_special_tokens=False)))\n",
    "print()\n",
    "\n",
    "# Show what BART actually sees after tokenization\n",
    "tokens = tokenizer.encode(simple_truncated, add_special_tokens=False)\n",
    "decoded_back = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(\"What BART actually processes:\")\n",
    "print(repr(decoded_back))\n",
    "print()\n",
    "\n",
    "print(\"=== ISSUES IDENTIFIED ===\")\n",
    "print(\"1. Cut off mid-sentence: 'By accessing and using this service, you agree to be bound by the terms...'\")\n",
    "print(\"2. Lost document type identifier: 'LEGAL AGREEMENT' is preserved, but context is lost\")  \n",
    "print(\"3. Character count ≠ token count: 203 characters ≈ ~50 tokens (varies by content)\")\n",
    "print(\"4. Classification might fail: Incomplete context about legal terms and obligations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
