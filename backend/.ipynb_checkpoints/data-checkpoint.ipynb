{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbb46b0",
   "metadata": {},
   "source": [
    "# Document Classification with BART-Large-MNLI\n",
    "\n",
    "This notebook explores document classification using Facebook's BART-Large-MNLI model for our Smart Document Classifier API.\n",
    "\n",
    "We'll use the **pipeline approach** (Option 1) as it's more suitable for our FastAPI integration:\n",
    "- Simpler implementation\n",
    "- Better error handling\n",
    "- Built-in optimizations\n",
    "- Easier to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5ccf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenshinluo/.pyenv/versions/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f8374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART-Large-MNLI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the zero-shot classification pipeline with BART-Large-MNLI\n",
    "print(\"Loading BART-Large-MNLI model...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbc92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document categories: ['Technical Documentation', 'Business Proposal', 'Legal Document', 'Academic Paper', 'General Article', 'Other']\n",
      "Total categories: 6\n"
     ]
    }
   ],
   "source": [
    "# Define document categories for classification\n",
    "DOCUMENT_CATEGORIES = [\n",
    "    \"Technical Documentation\",\n",
    "    \"Business Proposal\", \n",
    "    \"Legal Document\",\n",
    "    \"Academic Paper\",\n",
    "    \"General Article\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "print(f\"Document categories: {DOCUMENT_CATEGORIES}\")\n",
    "print(f\"Total categories: {len(DOCUMENT_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7db91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document classification function created!\n"
     ]
    }
   ],
   "source": [
    "def classify_document(text: str, categories: List[str] = DOCUMENT_CATEGORIES) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classify a document using BART-Large-MNLI zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        categories: List of possible categories\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Truncate text if too long (BART has token limits)\n",
    "        max_length = 1000  # Adjust based on performance needs\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + \"...\"\n",
    "        \n",
    "        # Perform classification\n",
    "        start_time = time.time()\n",
    "        result = classifier(text, categories)\n",
    "        inference_time = time.time() - start_time\n",
    "        print(result)\n",
    "        \n",
    "        # Format results\n",
    "        classification_result = {\n",
    "            \"predicted_category\": result[\"labels\"][0],\n",
    "            \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "            \"all_scores\": {\n",
    "                label: round(score, 4) \n",
    "                for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "            },\n",
    "            \"inference_time\": round(inference_time, 3),\n",
    "            \"model_used\": \"facebook/bart-large-mnli\"\n",
    "        }\n",
    "        \n",
    "        return classification_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"predicted_category\": None,\n",
    "            \"confidence_score\": 0.0\n",
    "        }\n",
    "\n",
    "print(\"Document classification function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b50cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing document classification...\n",
      "{'sequence': '\\nThis document outlines the technical specifications for implementing a RESTful API \\nusing FastAPI framework. The API includes endpoints for document upload, processing, \\nand classification. Key components include SQLAlchemy for database operations, \\nPydantic for data validation, and uvicorn as the ASGI server.\\n', 'labels': ['Technical Documentation', 'General Article', 'Other', 'Academic Paper', 'Business Proposal', 'Legal Document'], 'scores': [0.8104618191719055, 0.07211217284202576, 0.06284535676240921, 0.022135350853204727, 0.018267560750246048, 0.014177760109305382]}\n",
      "\n",
      "Classification Result:\n"
     ]
    }
   ],
   "source": [
    "# Test the classification with a sample document\n",
    "sample_text = \"\"\"\n",
    "This document outlines the technical specifications for implementing a RESTful API \n",
    "using FastAPI framework. The API includes endpoints for document upload, processing, \n",
    "and classification. Key components include SQLAlchemy for database operations, \n",
    "Pydantic for data validation, and uvicorn as the ASGI server.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing document classification...\")\n",
    "result = classify_document(sample_text)\n",
    "# print(\"\\nClassification Result:\")\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dac667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some documents from our dataset\n",
    "dataset_path = \"../ml/Dataset/\"\n",
    "\n",
    "# Read a few sample documents\n",
    "sample_files = [\n",
    "    \"python_doc.txt\",\n",
    "    \"How I use LLMs as a staff engineer.txt\", \n",
    "    \"compujai.txt\"\n",
    "]\n",
    "\n",
    "print(\"Testing classification on existing dataset documents:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename in sample_files:\n",
    "    filepath = os.path.join(dataset_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"\\nFile: {filename}\")\n",
    "        print(f\"Content preview: {content[:100]}...\")\n",
    "        \n",
    "        result = classify_document(content)\n",
    "        print(f\"Predicted Category: {result['predicted_category']}\")\n",
    "        print(f\"Confidence: {result['confidence_score']}\")\n",
    "        print(f\"Inference Time: {result['inference_time']}s\")\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed16b",
   "metadata": {},
   "source": [
    "## Creating ML Module for FastAPI Integration\n",
    "\n",
    "Now let's create the classifier module that will be integrated into our FastAPI backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cbcdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create the ML classifier module for FastAPI integration\n",
    "classifier_module_code = '''\n",
    "\"\"\"\n",
    "Document Classifier Module using BART-Large-MNLI\n",
    "For Smart Document Classifier FastAPI Application\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentClassifier:\n",
    "    \"\"\"Document classifier using Facebook's BART-Large-MNLI model\"\"\"\n",
    "    \n",
    "    CATEGORIES = [\n",
    "        \"Technical Documentation\",\n",
    "        \"Business Proposal\", \n",
    "        \"Legal Document\",\n",
    "        \"Academic Paper\",\n",
    "        \"General Article\",\n",
    "        \"Other\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier\"\"\"\n",
    "        self.classifier = None\n",
    "        self.model_name = \"facebook/bart-large-mnli\"\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the BART-Large-MNLI model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading {self.model_name} model...\")\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\", \n",
    "                model=self.model_name,\n",
    "                device=-1  # Use CPU, change to 0 for GPU\n",
    "            )\n",
    "            self.is_loaded = True\n",
    "            logger.info(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def classify(self, text: str, categories: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify a document\n",
    "        \n",
    "        Args:\n",
    "            text: Document text to classify\n",
    "            categories: Optional custom categories (defaults to self.CATEGORIES)\n",
    "            \n",
    "        Returns:\n",
    "            Classification results with confidence scores\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "            \n",
    "        if not text or not text.strip():\n",
    "            return {\n",
    "                \"error\": \"Empty text provided\",\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "            \n",
    "        categories = categories or self.CATEGORIES\n",
    "        \n",
    "        try:\n",
    "            # Truncate text if too long (BART token limit ~1024)\n",
    "            max_length = 800  # Conservative limit for better performance\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "                logger.info(f\"Text truncated to {max_length} characters\")\n",
    "            \n",
    "            # Perform classification\n",
    "            start_time = time.time()\n",
    "            result = self.classifier(text, categories)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Format results\n",
    "            classification_result = {\n",
    "                \"predicted_category\": result[\"labels\"][0],\n",
    "                \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "                \"all_scores\": {\n",
    "                    label: round(score, 4) \n",
    "                    for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "                },\n",
    "                \"inference_time\": round(inference_time, 3),\n",
    "                \"model_used\": self.model_name,\n",
    "                \"text_length\": len(text)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Classification completed: {result['labels'][0]} ({result['scores'][0]:.4f})\")\n",
    "            return classification_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "\n",
    "# Global classifier instance (singleton pattern)\n",
    "_classifier_instance = None\n",
    "\n",
    "def get_classifier() -> DocumentClassifier:\n",
    "    \"\"\"Get or create the global classifier instance\"\"\"\n",
    "    global _classifier_instance\n",
    "    if _classifier_instance is None:\n",
    "        _classifier_instance = DocumentClassifier()\n",
    "    return _classifier_instance\n",
    "\n",
    "def classify_document_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience function to classify document text\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        \n",
    "    Returns:\n",
    "        Classification results\n",
    "    \"\"\"\n",
    "    classifier = get_classifier()\n",
    "    return classifier.classify(text)\n",
    "'''\n",
    "\n",
    "# Write the module to a file\n",
    "with open('../backend/ml_classifier.py', 'w') as f:\n",
    "    f.write(classifier_module_code)\n",
    "\n",
    "print(\"✅ ML classifier module created at: backend/ml_classifier.py\")\n",
    "print(\"📦 Module includes:\")\n",
    "print(\"   - DocumentClassifier class\")\n",
    "print(\"   - Singleton pattern for model loading\")\n",
    "print(\"   - Error handling and logging\")\n",
    "print(\"   - Performance optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc8621",
   "metadata": {},
   "source": [
    "## Fixing Multiprocessing Resource Warning\n",
    "\n",
    "The warning about leaked semaphore objects occurs because the transformers library uses multiprocessing resources that aren't properly cleaned up on shutdown. We've implemented proper resource management:\n",
    "\n",
    "1. **Added cleanup methods** to the DocumentClassifier class\n",
    "2. **Registered atexit handlers** to cleanup on process termination  \n",
    "3. **Added FastAPI shutdown event** to cleanup ML resources\n",
    "4. **Explicit resource management** with garbage collection and PyTorch cache clearing\n",
    "\n",
    "This prevents the resource tracker warning: `resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
