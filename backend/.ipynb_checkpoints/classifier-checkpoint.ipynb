{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbb46b0",
   "metadata": {},
   "source": [
    "# Document Classification with BART-Large-MNLI\n",
    "\n",
    "This notebook explores document classification using Facebook's BART-Large-MNLI model for our Smart Document Classifier API.\n",
    "\n",
    "We'll use the **pipeline approach** (Option 1) as it's more suitable for our FastAPI integration:\n",
    "- Simpler implementation\n",
    "- Better error handling\n",
    "- Built-in optimizations\n",
    "- Easier to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28923d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb630d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install additional packages for model comparison and visualization\n",
    "!pip install matplotlib seaborn pandas scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4544f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import additional libraries for model comparison\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for model comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import time  # Fix the time import\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5ccf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenshinluo/.pyenv/versions/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f8374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART-Large-MNLI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the zero-shot classification pipeline with BART-Large-MNLI\n",
    "print(\"Loading BART-Large-MNLI model...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbc92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document categories: ['Technical Documentation', 'Business Proposal', 'Legal Document', 'Academic Paper', 'General Article', 'Other']\n",
      "Total categories: 6\n"
     ]
    }
   ],
   "source": [
    "# Define document categories for classification\n",
    "DOCUMENT_CATEGORIES = [\n",
    "    \"Technical Documentation\",\n",
    "    \"Business Proposal\", \n",
    "    \"Legal Document\",\n",
    "    \"Academic Paper\",\n",
    "    \"General Article\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "print(f\"Document categories: {DOCUMENT_CATEGORIES}\")\n",
    "print(f\"Total categories: {len(DOCUMENT_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document classification function created!\n"
     ]
    }
   ],
   "source": [
    "def classify_document(text: str, categories: List[str] = DOCUMENT_CATEGORIES) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classify a document using BART-Large-MNLI zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        categories: List of possible categories\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # IMPROVED: Use tokenizer-based truncation instead of character truncation\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "        \n",
    "        # Count actual tokens, not characters\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        max_tokens = 800  # Conservative limit for BART\n",
    "        \n",
    "        if len(tokens) > max_tokens:\n",
    "            # Proper token-based truncation\n",
    "            truncated_tokens = tokens[:max_tokens]\n",
    "            text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "            print(f\"⚠️  Text truncated from {len(tokens)} to {max_tokens} tokens\")\n",
    "        \n",
    "        # Perform classification\n",
    "        start_time = time.time()\n",
    "        result = classifier(text, categories)\n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"📊 Classification result: {result['labels'][0]} ({result['scores'][0]:.4f})\")\n",
    "        \n",
    "        # Format results\n",
    "        classification_result = {\n",
    "            \"predicted_category\": result[\"labels\"][0],\n",
    "            \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "            \"all_scores\": {\n",
    "                label: round(score, 4) \n",
    "                for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "            },\n",
    "            \"inference_time\": round(inference_time, 3),\n",
    "            \"model_used\": \"facebook/bart-large-mnli\",\n",
    "            \"token_count\": len(tokens),\n",
    "            \"was_truncated\": len(tokens) > max_tokens\n",
    "        }\n",
    "        \n",
    "        return classification_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"predicted_category\": None,\n",
    "            \"confidence_score\": 0.0\n",
    "        }\n",
    "\n",
    "print(\"✅ Updated document classification function with proper tokenizer-based truncation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old vs new truncation methods\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for proper token counting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "test_doc = \"\"\"\n",
    "LEGAL AGREEMENT - SOFTWARE LICENSE TERMS\n",
    "\n",
    "IMPORTANT: Please read these terms and conditions carefully before using our software.\n",
    "\n",
    "1. GRANT OF LICENSE\n",
    "Subject to the terms and conditions of this Agreement, Company hereby grants to you a limited, non-exclusive, non-transferable license to use the software solely for your internal business purposes.\n",
    "\n",
    "2. RESTRICTIONS\n",
    "You may not: (a) modify, adapt, or create derivative works; (b) reverse engineer, decompile, or disassemble; (c) remove or alter any proprietary notices; (d) distribute, sublicense, or transfer the software.\n",
    "\n",
    "3. INTELLECTUAL PROPERTY\n",
    "All intellectual property rights in and to the software remain the exclusive property of Company. No rights are granted except as expressly set forth herein.\n",
    "\n",
    "4. WARRANTY DISCLAIMER\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.\n",
    "\n",
    "5. LIMITATION OF LIABILITY\n",
    "IN NO EVENT SHALL COMPANY BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING OUT OF OR RELATING TO THIS AGREEMENT.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 TRUNCATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original document: {len(test_doc)} characters, {len(tokenizer.encode(test_doc))} tokens\")\n",
    "print()\n",
    "\n",
    "# OLD METHOD (Character-based - WRONG)\n",
    "old_truncated = test_doc[:300] + \"...\"\n",
    "print(\"❌ OLD METHOD (Character truncation at 300 chars):\")\n",
    "print(f\"Result: {len(old_truncated)} chars, {len(tokenizer.encode(old_truncated))} tokens\")\n",
    "print(f\"Preview: {repr(old_truncated[:100])}...\")\n",
    "print()\n",
    "\n",
    "# NEW METHOD (Token-based - CORRECT)\n",
    "tokens = tokenizer.encode(test_doc, add_special_tokens=False)\n",
    "max_tokens = 80  # Small limit for demo\n",
    "if len(tokens) > max_tokens:\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    new_truncated = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "else:\n",
    "    new_truncated = test_doc\n",
    "\n",
    "print(\"✅ NEW METHOD (Token truncation at 80 tokens):\")\n",
    "print(f\"Result: {len(new_truncated)} chars, {len(tokenizer.encode(new_truncated))} tokens\")\n",
    "print(f\"Preview: {repr(new_truncated[:100])}...\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 KEY DIFFERENCES:\")\n",
    "print(\"1. Old method cuts mid-sentence, new method respects token boundaries\")\n",
    "print(\"2. Old method doesn't account for tokenization differences\")\n",
    "print(\"3. New method ensures exact token count for BART model\")\n",
    "print(\"4. New method preserves more meaningful content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b50cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing document classification...\n",
      "{'sequence': '\\nThis document outlines the technical specifications for implementing a RESTful API \\nusing FastAPI framework. The API includes endpoints for document upload, processing, \\nand classification. Key components include SQLAlchemy for database operations, \\nPydantic for data validation, and uvicorn as the ASGI server.\\n', 'labels': ['Technical Documentation', 'General Article', 'Other', 'Academic Paper', 'Business Proposal', 'Legal Document'], 'scores': [0.8104618191719055, 0.07211217284202576, 0.06284535676240921, 0.022135350853204727, 0.018267560750246048, 0.014177760109305382]}\n",
      "\n",
      "Classification Result:\n"
     ]
    }
   ],
   "source": [
    "# Test the classification with a sample document\n",
    "sample_text = \"\"\"\n",
    "This document outlines the technical specifications for implementing a RESTful API \n",
    "using FastAPI framework. The API includes endpoints for document upload, processing, \n",
    "and classification. Key components include SQLAlchemy for database operations, \n",
    "Pydantic for data validation, and uvicorn as the ASGI server.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing document classification...\")\n",
    "result = classify_document(sample_text)\n",
    "# print(\"\\nClassification Result:\")\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dac667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some documents from our dataset\n",
    "dataset_path = \"../ml/Dataset/\"\n",
    "\n",
    "# Read a few sample documents\n",
    "sample_files = [\n",
    "    \"python_doc.txt\",\n",
    "    \"How I use LLMs as a staff engineer.txt\", \n",
    "    \"compujai.txt\"\n",
    "]\n",
    "\n",
    "print(\"Testing classification on existing dataset documents:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename in sample_files:\n",
    "    filepath = os.path.join(dataset_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"\\nFile: {filename}\")\n",
    "        print(f\"Content preview: {content[:100]}...\")\n",
    "        \n",
    "        result = classify_document(content)\n",
    "        print(f\"Predicted Category: {result['predicted_category']}\")\n",
    "        print(f\"Confidence: {result['confidence_score']}\")\n",
    "        print(f\"Inference Time: {result['inference_time']}s\")\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed16b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating ML Module for FastAPI Integration\n",
    "\n",
    "Now let's create the classifier module that will be integrated into our FastAPI backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cbcdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mERROR: Failed to build installable wheels for some pyproject.toml based projects (pyzmq). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create the ML classifier module for FastAPI integration\n",
    "classifier_module_code = '''\n",
    "\"\"\"\n",
    "Document Classifier Module using BART-Large-MNLI\n",
    "For Smart Document Classifier FastAPI Application\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentClassifier:\n",
    "    \"\"\"Document classifier using Facebook's BART-Large-MNLI model\"\"\"\n",
    "    \n",
    "    CATEGORIES = [\n",
    "        \"Technical Documentation\",\n",
    "        \"Business Proposal\", \n",
    "        \"Legal Document\",\n",
    "        \"Academic Paper\",\n",
    "        \"General Article\",\n",
    "        \"Other\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier\"\"\"\n",
    "        self.classifier = None\n",
    "        self.model_name = \"facebook/bart-large-mnli\"\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the BART-Large-MNLI model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading {self.model_name} model...\")\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\", \n",
    "                model=self.model_name,\n",
    "                device=-1  # Use CPU, change to 0 for GPU\n",
    "            )\n",
    "            self.is_loaded = True\n",
    "            logger.info(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def classify(self, text: str, categories: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify a document\n",
    "        \n",
    "        Args:\n",
    "            text: Document text to classify\n",
    "            categories: Optional custom categories (defaults to self.CATEGORIES)\n",
    "            \n",
    "        Returns:\n",
    "            Classification results with confidence scores\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "            \n",
    "        if not text or not text.strip():\n",
    "            return {\n",
    "                \"error\": \"Empty text provided\",\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "            \n",
    "        categories = categories or self.CATEGORIES\n",
    "        \n",
    "        try:\n",
    "            # Truncate text if too long (BART token limit ~1024)\n",
    "            max_length = 800  # Conservative limit for better performance\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "                logger.info(f\"Text truncated to {max_length} characters\")\n",
    "            \n",
    "            # Perform classification\n",
    "            start_time = time.time()\n",
    "            result = self.classifier(text, categories)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Format results\n",
    "            classification_result = {\n",
    "                \"predicted_category\": result[\"labels\"][0],\n",
    "                \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "                \"all_scores\": {\n",
    "                    label: round(score, 4) \n",
    "                    for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "                },\n",
    "                \"inference_time\": round(inference_time, 3),\n",
    "                \"model_used\": self.model_name,\n",
    "                \"text_length\": len(text)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Classification completed: {result['labels'][0]} ({result['scores'][0]:.4f})\")\n",
    "            return classification_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }\n",
    "\n",
    "# Global classifier instance (singleton pattern)\n",
    "_classifier_instance = None\n",
    "\n",
    "def get_classifier() -> DocumentClassifier:\n",
    "    \"\"\"Get or create the global classifier instance\"\"\"\n",
    "    global _classifier_instance\n",
    "    if _classifier_instance is None:\n",
    "        _classifier_instance = DocumentClassifier()\n",
    "    return _classifier_instance\n",
    "\n",
    "def classify_document_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience function to classify document text\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        \n",
    "    Returns:\n",
    "        Classification results\n",
    "    \"\"\"\n",
    "    classifier = get_classifier()\n",
    "    return classifier.classify(text)\n",
    "'''\n",
    "\n",
    "# Write the module to a file\n",
    "with open('../backend/ml_classifier.py', 'w') as f:\n",
    "    f.write(classifier_module_code)\n",
    "\n",
    "print(\"✅ ML classifier module created at: backend/ml_classifier.py\")\n",
    "print(\"📦 Module includes:\")\n",
    "print(\"   - DocumentClassifier class\")\n",
    "print(\"   - Singleton pattern for model loading\")\n",
    "print(\"   - Error handling and logging\")\n",
    "print(\"   - Performance optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc8621",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fixing Multiprocessing Resource Warning\n",
    "\n",
    "The warning about leaked semaphore objects occurs because the transformers library uses multiprocessing resources that aren't properly cleaned up on shutdown. We've implemented proper resource management:\n",
    "\n",
    "1. **Added cleanup methods** to the DocumentClassifier class\n",
    "2. **Registered atexit handlers** to cleanup on process termination  \n",
    "3. **Added FastAPI shutdown event** to cleanup ML resources\n",
    "4. **Explicit resource management** with garbage collection and PyTorch cache clearing\n",
    "\n",
    "This prevents the resource tracker warning: `resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a6970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Better Text Truncation Strategies\n",
    "\n",
    "The current simple truncation `text[:1000]` is problematic because:\n",
    "\n",
    "1. **Character vs Token mismatch** - BART uses BPE tokenization, not character counting\n",
    "2. **Loses important context** - May cut mid-sentence or lose document structure  \n",
    "3. **Suboptimal for classification** - Beginning might not contain key classification signals\n",
    "4. **No intelligent splitting** - Doesn't respect word/sentence boundaries\n",
    "\n",
    "Let's implement better strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load BART tokenizer to properly count tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "\n",
    "def smart_truncate_text(text: str, max_tokens: int = 800) -> str:\n",
    "    \"\"\"\n",
    "    Intelligently truncate text for BART classification\n",
    "    \n",
    "    Strategies:\n",
    "    1. Use actual tokenizer to count tokens, not characters\n",
    "    2. Take beginning + end of document (sandwich approach)  \n",
    "    3. Preserve sentence boundaries\n",
    "    4. Include document structure clues (titles, headers)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: Simple tokenizer-based truncation\n",
    "    def tokenizer_truncate(text: str, max_tokens: int) -> str:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "        \n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        return tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Strategy 2: Sandwich approach (beginning + end)\n",
    "    def sandwich_truncate(text: str, max_tokens: int) -> str:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "            \n",
    "        # Take 60% from beginning, 40% from end\n",
    "        start_tokens = int(max_tokens * 0.6)\n",
    "        end_tokens = max_tokens - start_tokens\n",
    "        \n",
    "        beginning = tokenizer.decode(tokens[:start_tokens], skip_special_tokens=True)\n",
    "        ending = tokenizer.decode(tokens[-end_tokens:], skip_special_tokens=True)\n",
    "        \n",
    "        return f\"{beginning}\\n\\n[...document continues...]\\n\\n{ending}\"\n",
    "    \n",
    "    # Strategy 3: Smart chunking (preserve sentences)\n",
    "    def smart_chunk_truncate(text: str, max_tokens: int) -> str:\n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        result_tokens = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            \n",
    "            if current_length + len(sentence_tokens) <= max_tokens:\n",
    "                result_tokens.extend(sentence_tokens)\n",
    "                current_length += len(sentence_tokens)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if result_tokens:\n",
    "            return tokenizer.decode(result_tokens, skip_special_tokens=True)\n",
    "        else:\n",
    "            # Fallback to simple truncation if first sentence is too long\n",
    "            return tokenizer_truncate(text, max_tokens)\n",
    "    \n",
    "    # Choose strategy based on document characteristics\n",
    "    token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "    \n",
    "    if token_count <= max_tokens:\n",
    "        return text\n",
    "    elif token_count > max_tokens * 3:  # Very long document\n",
    "        return sandwich_truncate(text, max_tokens)\n",
    "    else:  # Moderately long document\n",
    "        return smart_chunk_truncate(text, max_tokens)\n",
    "\n",
    "# Test the different approaches\n",
    "test_text = \"\"\"\n",
    "# Technical Documentation: FastAPI Implementation Guide\n",
    "\n",
    "This comprehensive guide covers the implementation of a FastAPI web application for document classification.\n",
    "\n",
    "## Architecture Overview\n",
    "The system uses a modular architecture with the following components:\n",
    "- FastAPI framework for REST API endpoints\n",
    "- SQLAlchemy ORM for database operations  \n",
    "- Pydantic models for data validation\n",
    "- BART-Large-MNLI for document classification\n",
    "- Uvicorn ASGI server for deployment\n",
    "\n",
    "## Implementation Details\n",
    "The core application consists of several modules that work together to provide document classification capabilities.\n",
    "\n",
    "### Database Layer\n",
    "The database layer uses SQLAlchemy to manage document metadata and classification results.\n",
    "\n",
    "### ML Classification\n",
    "The machine learning component uses Facebook's BART-Large-MNLI model for zero-shot classification.\n",
    "\n",
    "## Performance Considerations\n",
    "For production deployment, consider the following optimizations:\n",
    "- Model caching and singleton patterns\n",
    "- Async processing for better throughput  \n",
    "- Resource cleanup to prevent memory leaks\n",
    "- Proper error handling and logging\n",
    "\n",
    "## Conclusion\n",
    "This FastAPI implementation provides a robust foundation for document classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text length:\", len(test_text), \"characters\")\n",
    "print(\"Original token count:\", len(tokenizer.encode(test_text, add_special_tokens=False)), \"tokens\")\n",
    "print()\n",
    "\n",
    "# Test different truncation methods\n",
    "truncated = smart_truncate_text(test_text, max_tokens=100)\n",
    "print(\"Smart truncated length:\", len(tokenizer.encode(truncated, add_special_tokens=False)), \"tokens\")\n",
    "print(\"Smart truncated text:\")\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd927069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problem with simple character truncation\n",
    "sample_document = \"\"\"\n",
    "LEGAL AGREEMENT - TERMS OF SERVICE\n",
    "\n",
    "IMPORTANT: Please read these terms carefully before using our service.\n",
    "\n",
    "1. ACCEPTANCE OF TERMS\n",
    "By accessing and using this service, you agree to be bound by the terms and conditions outlined in this agreement.\n",
    "\n",
    "2. SERVICE DESCRIPTION  \n",
    "Our document classification service uses artificial intelligence to automatically categorize uploaded documents into predefined categories.\n",
    "\n",
    "3. USER OBLIGATIONS\n",
    "Users must ensure that uploaded documents do not contain:\n",
    "- Confidential or proprietary information\n",
    "- Personal identifying information (PII) \n",
    "- Copyrighted material without permission\n",
    "- Malicious code or harmful content\n",
    "\n",
    "4. LIMITATION OF LIABILITY\n",
    "In no event shall the company be liable for any indirect, incidental, special, consequential, or punitive damages.\n",
    "\n",
    "5. TERMINATION\n",
    "We reserve the right to terminate or suspend access to our service immediately, without prior notice.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== PROBLEM WITH SIMPLE CHARACTER TRUNCATION ===\")\n",
    "print()\n",
    "\n",
    "# Current problematic approach (character-based)\n",
    "simple_truncated = sample_document[:200] + \"...\"\n",
    "print(\"Simple character truncation (200 chars):\")\n",
    "print(repr(simple_truncated))\n",
    "print()\n",
    "\n",
    "# Show what happens with tokenization\n",
    "print(\"Tokens in simple truncated text:\", len(tokenizer.encode(simple_truncated, add_special_tokens=False)))\n",
    "print()\n",
    "\n",
    "# Show what BART actually sees after tokenization\n",
    "tokens = tokenizer.encode(simple_truncated, add_special_tokens=False)\n",
    "decoded_back = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(\"What BART actually processes:\")\n",
    "print(repr(decoded_back))\n",
    "print()\n",
    "\n",
    "print(\"=== ISSUES IDENTIFIED ===\")\n",
    "print(\"1. Cut off mid-sentence: 'By accessing and using this service, you agree to be bound by the terms...'\")\n",
    "print(\"2. Lost document type identifier: 'LEGAL AGREEMENT' is preserved, but context is lost\")  \n",
    "print(\"3. Character count ≠ token count: 203 characters ≈ ~50 tokens (varies by content)\")\n",
    "print(\"4. Classification might fail: Incomplete context about legal terms and obligations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b217e5",
   "metadata": {},
   "source": [
    "# Multi-Model Comparison for Document Classification\n",
    "\n",
    "Now let's implement a comprehensive comparison of different pre-trained models to find the best performer for our document classification task.\n",
    "\n",
    "## Models to Compare:\n",
    "- **facebook/bart-large-mnli** (current) - BART Large MNLI\n",
    "- **distilbert-base-uncased** - Distilled BERT \n",
    "- **bert-base-uncased** - BERT Base\n",
    "- **roberta-base** - RoBERTa Base\n",
    "- **microsoft/DialoGPT-medium** - Alternative conversational model\n",
    "- **sentence-transformers/all-MiniLM-L6-v2** - Sentence transformer for embeddings\n",
    "\n",
    "We'll test these models on documents from `data/Dataset/` and compare:\n",
    "- Classification accuracy\n",
    "- Inference time\n",
    "- Confidence scores\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a6ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model comparison class initialized!\n"
     ]
    }
   ],
   "source": [
    "# Define comprehensive model comparison class\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import time as time_module  # Fix the import conflict\n",
    "\n",
    "class ModelComparison:\n",
    "    \"\"\"Compare different pre-trained models for document classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'BART-Large-MNLI': 'facebook/bart-large-mnli',\n",
    "            'DistilBERT': 'distilbert-base-uncased', \n",
    "            'BERT-Base': 'bert-base-uncased',\n",
    "            'RoBERTa-Base': 'roberta-base',\n",
    "            'DeBERTa-Base': 'microsoft/deberta-base'  # Alternative to DialoGPT\n",
    "        }\n",
    "        \n",
    "        self.categories = [\n",
    "            \"Technical Documentation\",\n",
    "            \"Business Proposal\", \n",
    "            \"Legal Document\",\n",
    "            \"Academic Paper\",\n",
    "            \"General Article\",\n",
    "            \"Other\"\n",
    "        ]\n",
    "        \n",
    "        self.classifiers = {}\n",
    "        self.results = []\n",
    "        \n",
    "    def load_model(self, model_name, model_path):\n",
    "        \"\"\"Load a specific model for classification\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {model_name} ({model_path})...\")\n",
    "            \n",
    "            # Use different approaches for different model types\n",
    "            if 'bart-large-mnli' in model_path:\n",
    "                # BART uses zero-shot classification\n",
    "                classifier = pipeline(\"zero-shot-classification\", model=model_path, device=-1)\n",
    "            else:\n",
    "                # Try zero-shot first for other models\n",
    "                try:\n",
    "                    classifier = pipeline(\"zero-shot-classification\", model=model_path, device=-1)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Zero-shot not available for {model_name}, trying text-classification...\")\n",
    "                    # Some models might not support zero-shot, skip them for now\n",
    "                    print(f\"  ⚠️ Skipping {model_name}: {str(e)}\")\n",
    "                    return False\n",
    "            \n",
    "            self.classifiers[model_name] = {\n",
    "                'pipeline': classifier,\n",
    "                'model_path': model_path,\n",
    "                'type': 'zero-shot'\n",
    "            }\n",
    "            print(f\"  ✅ {model_name} loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to load {model_name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def classify_with_model(self, text, model_name):\n",
    "        \"\"\"Classify text with a specific model\"\"\"\n",
    "        if model_name not in self.classifiers:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            classifier_info = self.classifiers[model_name]\n",
    "            classifier = classifier_info['pipeline']\n",
    "            \n",
    "            # Truncate text for model limits\n",
    "            max_length = 512 if 'bert' in model_name.lower() else 800\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "            \n",
    "            start_time = time_module.time()\n",
    "            \n",
    "            # Zero-shot classification\n",
    "            result = classifier(text, self.categories)\n",
    "            predicted_category = result['labels'][0]\n",
    "            confidence = result['scores'][0]\n",
    "            all_scores = {label: score for label, score in zip(result['labels'], result['scores'])}\n",
    "            \n",
    "            inference_time = time_module.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'predicted_category': predicted_category,\n",
    "                'confidence': confidence,\n",
    "                'all_scores': all_scores,\n",
    "                'inference_time': inference_time,\n",
    "                'model': model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Classification failed for {model_name}: {str(e)}\")\n",
    "            return {\n",
    "                'predicted_category': 'Other',\n",
    "                'confidence': 0.0,\n",
    "                'all_scores': {'Other': 0.0},\n",
    "                'inference_time': 0.0,\n",
    "                'model': model_name,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all models for comparison\"\"\"\n",
    "        loaded_count = 0\n",
    "        for model_name, model_path in self.models.items():\n",
    "            if self.load_model(model_name, model_path):\n",
    "                loaded_count += 1\n",
    "            time_module.sleep(1)  # Brief pause between model loads\n",
    "        \n",
    "        print(f\"\\n✅ Successfully loaded {loaded_count}/{len(self.models)} models\")\n",
    "        return loaded_count > 0\n",
    "\n",
    "# Initialize the comparison class\n",
    "model_comparison = ModelComparison()\n",
    "print(\"Model comparison class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a49c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading documents from dataset...\n",
      "Found 17 text files in dataset directory\n",
      "  ✅ Loaded: Python Patterns .txt (3695 chars) -> Technical Documentation\n",
      "  ✅ Loaded: Proposal for the Implementation of DAO for Enhanced Data Governance and Collaboritive Research in Genomic Sequencing.txt (6537 chars) -> Business Proposal\n",
      "  ✅ Loaded: Chat UI Pattern.txt (2427 chars) -> Other\n",
      "  ✅ Loaded: Consolidated Paperclips.txt (4544 chars) -> Academic Paper\n",
      "  ✅ Loaded: Unveiling the Universe's Secrets.txt (4174 chars) -> Other\n",
      "  ✅ Loaded: Lightweight Authenticated Cryptography; Balancing Security and Efficiency in Resource-Constrained Environments.txt (5581 chars) -> Other\n",
      "  ✅ Loaded: Celestial Edge.txt (6186 chars) -> Other\n",
      "  ✅ Loaded: Agreement-Regarding-Quantum-Leap.txt (11431 chars) -> Legal Document\n",
      "  ✅ Loaded: AugmentAI - Empower through intelligent automation.txt (9085 chars) -> Other\n",
      "  ✅ Loaded: python_doc.txt (2774 chars) -> Technical Documentation\n",
      "  ✅ Loaded: compujai.txt (7747 chars) -> Other\n",
      "  ✅ Loaded: Charting the Landscape of Electroweak Symmetry.txt (7761 chars) -> Other\n",
      "  ✅ Loaded: How I use LLMs as a staff engineer.txt (5984 chars) -> Other\n",
      "  ✅ Loaded: DreamWeaver5000.txt (2649 chars) -> Other\n",
      "  ✅ Loaded: Multifaceted Exploration.txt (4639 chars) -> Other\n",
      "  ✅ Loaded: A Collection of Life.txt (776 chars) -> Other\n",
      "  ✅ Loaded: Dust and Dreams.txt (1721 chars) -> Other\n",
      "\n",
      "📊 Loaded 17 documents for testing\n",
      "\n",
      "📋 Document distribution:\n",
      "  Technical Documentation: 2 documents\n",
      "  Business Proposal: 1 documents\n",
      "  Other: 12 documents\n",
      "  Academic Paper: 1 documents\n",
      "  Legal Document: 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents from data/Dataset directory\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_documents_from_dataset(dataset_path=\"../data/Dataset\", max_docs=20):\n",
    "    \"\"\"Load documents from the dataset directory\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Get all text files from dataset (focus on .txt files for simplicity)\n",
    "    file_pattern = os.path.join(dataset_path, \"*.txt\")\n",
    "    all_files = glob.glob(file_pattern)\n",
    "    \n",
    "    print(f\"Found {len(all_files)} text files in dataset directory\")\n",
    "    \n",
    "    # Limit to max_docs for faster testing\n",
    "    selected_files = all_files[:max_docs] if len(all_files) > max_docs else all_files\n",
    "    \n",
    "    for filepath in selected_files:\n",
    "        try:\n",
    "            filename = os.path.basename(filepath)\n",
    "            \n",
    "            content = \"\"\n",
    "            \n",
    "            # Read text file with multiple encoding attempts\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding=encoding) as f:\n",
    "                        content = f.read()\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Only add documents with sufficient content\n",
    "            if content and len(content.strip()) > 100:\n",
    "                # Get expected category from filename (manual labeling for testing)\n",
    "                expected_category = predict_category_from_filename(filename)\n",
    "                \n",
    "                documents.append({\n",
    "                    'filename': filename,\n",
    "                    'content': content.strip(),\n",
    "                    'expected_category': expected_category,\n",
    "                    'file_type': 'txt'\n",
    "                })\n",
    "                print(f\"  ✅ Loaded: {filename} ({len(content)} chars) -> {expected_category}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️  Skipped: {filename} (insufficient content)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {filepath}: {str(e)}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def predict_category_from_filename(filename):\n",
    "    \"\"\"Predict expected category based on filename patterns\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    \n",
    "    # Technical documentation patterns\n",
    "    if any(word in filename_lower for word in ['api', 'technical', 'programming', 'code', 'development', 'software', 'python', 'javascript', 'react', 'fastapi']):\n",
    "        return \"Technical Documentation\"\n",
    "    \n",
    "    # Business patterns  \n",
    "    elif any(word in filename_lower for word in ['business', 'proposal', 'strategy', 'marketing', 'company', 'startup']):\n",
    "        return \"Business Proposal\"\n",
    "    \n",
    "    # Legal patterns\n",
    "    elif any(word in filename_lower for word in ['legal', 'agreement', 'contract', 'terms', 'privacy', 'license']):\n",
    "        return \"Legal Document\"\n",
    "    \n",
    "    # Academic patterns\n",
    "    elif any(word in filename_lower for word in ['paper', 'research', 'study', 'analysis', 'academic', 'journal']):\n",
    "        return \"Academic Paper\"\n",
    "    \n",
    "    # Article patterns\n",
    "    elif any(word in filename_lower for word in ['article', 'news', 'blog', 'story', 'guide']):\n",
    "        return \"General Article\"\n",
    "    \n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Load the documents\n",
    "print(\"🔍 Loading documents from dataset...\")\n",
    "test_documents = load_documents_from_dataset()\n",
    "print(f\"\\n📊 Loaded {len(test_documents)} documents for testing\")\n",
    "\n",
    "# Show summary\n",
    "if test_documents:\n",
    "    category_counts = {}\n",
    "    for doc in test_documents:\n",
    "        category = doc['expected_category']\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "    \n",
    "    print(\"\\n📋 Document distribution:\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  {category}: {count} documents\")\n",
    "else:\n",
    "    print(\"❌ No documents loaded. Please check the dataset path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fa7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive model comparison...\n",
      "============================================================\n",
      "Loading BART-Large-MNLI (facebook/bart-large-mnli)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ BART-Large-MNLI loaded successfully!\n",
      "✅ BART-Large-MNLI loaded successfully\n",
      "Loading DistilBERT (distilbert-base-uncased)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ DistilBERT loaded successfully!\n",
      "✅ DistilBERT loaded successfully\n",
      "Loading BERT-Base (bert-base-uncased)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ BERT-Base loaded successfully!\n",
      "✅ BERT-Base loaded successfully\n",
      "\n",
      "🔄 Running comparison on 17 documents with 3 models...\n",
      "\n",
      "📄 Document 1/8: Python Patterns .txt...\n",
      "  🤖 BART-Large-MNLI... ✅ Technical Documentat (0.785)\n",
      "  🤖 DistilBERT... ❌ Legal Document (0.168)\n",
      "  🤖 BERT-Base... ❌ Other (0.179)\n",
      "\n",
      "📄 Document 2/8: Proposal for the Implementation of DAO f...\n",
      "  🤖 BART-Large-MNLI... ✅ Business Proposal (0.346)\n",
      "  🤖 DistilBERT... ❌ Technical Documentat (0.168)\n",
      "  🤖 BERT-Base... ❌ Other (0.220)\n",
      "\n",
      "📄 Document 3/8: Chat UI Pattern.txt...\n",
      "  🤖 BART-Large-MNLI... ✅ Other (0.376)\n",
      "  🤖 DistilBERT... ❌ Technical Documentat (0.167)\n",
      "  🤖 BERT-Base... ✅ Other (0.171)\n",
      "\n",
      "📄 Document 4/8: Consolidated Paperclips.txt...\n",
      "  🤖 BART-Large-MNLI... ❌ Legal Document (0.582)\n",
      "  🤖 DistilBERT... ❌ Technical Documentat (0.168)\n",
      "  🤖 BERT-Base... ❌ Legal Document (0.183)\n",
      "\n",
      "📄 Document 5/8: Unveiling the Universe's Secrets.txt...\n",
      "  🤖 BART-Large-MNLI... ❌ Academic Paper (0.303)\n",
      "  🤖 DistilBERT... ❌ Technical Documentat (0.167)\n",
      "  🤖 BERT-Base... ❌ Business Proposal (0.185)\n",
      "\n",
      "📄 Document 6/8: Lightweight Authenticated Cryptography; ...\n",
      "  🤖 BART-Large-MNLI... ❌ Academic Paper (0.405)\n",
      "  🤖 DistilBERT... ❌ Legal Document (0.168)\n",
      "  🤖 BERT-Base... ✅ Other (0.178)\n",
      "\n",
      "📄 Document 7/8: Celestial Edge.txt...\n",
      "  🤖 BART-Large-MNLI... ❌ Business Proposal (0.601)\n",
      "  🤖 DistilBERT... ❌ Legal Document (0.168)\n",
      "  🤖 BERT-Base... ✅ Other (0.185)\n",
      "\n",
      "📄 Document 8/8: Agreement-Regarding-Quantum-Leap.txt...\n",
      "  🤖 BART-Large-MNLI... ✅ Legal Document (0.681)\n",
      "  🤖 DistilBERT... ✅ Legal Document (0.168)\n",
      "  🤖 BERT-Base... ❌ Other (0.205)\n",
      "\n",
      "✅ Comparison complete!\n",
      "📊 Tested 3 models on 8 documents\n",
      "\n",
      "📋 Results Summary:\n",
      "  BART-Large-MNLI: 0.500 accuracy (4/8), 0.510 confidence, 1.408s\n",
      "  DistilBERT: 0.125 accuracy (1/8), 0.168 confidence, 0.166s\n",
      "  BERT-Base: 0.375 accuracy (3/8), 0.188 confidence, 0.328s\n",
      "\n",
      "🎯 Results DataFrame created! Ready for visualization.\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive model comparison with results for visualization\n",
    "import pandas as pd\n",
    "import time as time_module\n",
    "\n",
    "print(\"🚀 Starting comprehensive model comparison...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load 3 models that we know work well\n",
    "models_to_test = {\n",
    "    'BART-Large-MNLI': 'facebook/bart-large-mnli',\n",
    "    'DistilBERT': 'distilbert-base-uncased', \n",
    "    'BERT-Base': 'bert-base-uncased'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "loaded_models = {}\n",
    "for model_name, model_path in models_to_test.items():\n",
    "    if model_comparison.load_model(model_name, model_path):\n",
    "        loaded_models[model_name] = model_path\n",
    "        print(f\"✅ {model_name} loaded successfully\")\n",
    "\n",
    "if loaded_models:\n",
    "    print(f\"\\n🔄 Running comparison on {len(test_documents)} documents with {len(loaded_models)} models...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Test on first 8 documents to avoid encoding issues\n",
    "    safe_documents = []\n",
    "    for doc in test_documents[:8]:\n",
    "        try:\n",
    "            # Test if document content can be processed safely\n",
    "            content_preview = doc['content'][:100].encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            safe_documents.append(doc)\n",
    "        except:\n",
    "            print(f\"⚠️ Skipping document with encoding issues: {doc['filename']}\")\n",
    "            continue\n",
    "    \n",
    "    for i, doc in enumerate(safe_documents):\n",
    "        print(f\"\\n📄 Document {i+1}/{len(safe_documents)}: {doc['filename'][:40]}...\")\n",
    "        \n",
    "        doc_results = {\n",
    "            'document': doc['filename'],\n",
    "            'expected_category': doc['expected_category'],\n",
    "            'content_length': len(doc['content']),\n",
    "            'file_type': doc['file_type']\n",
    "        }\n",
    "        \n",
    "        # Test each model\n",
    "        for model_name in loaded_models.keys():\n",
    "            print(f\"  🤖 {model_name}...\", end=\" \")\n",
    "            \n",
    "            try:\n",
    "                # Clean content to avoid encoding issues\n",
    "                clean_content = doc['content'].encode('utf-8', errors='ignore').decode('utf-8')\n",
    "                result = model_comparison.classify_with_model(clean_content, model_name)\n",
    "                \n",
    "                if result and 'error' not in result:\n",
    "                    doc_results[f\"{model_name}_prediction\"] = result['predicted_category']\n",
    "                    doc_results[f\"{model_name}_confidence\"] = result['confidence']\n",
    "                    doc_results[f\"{model_name}_time\"] = result['inference_time']\n",
    "                    doc_results[f\"{model_name}_correct\"] = (result['predicted_category'] == doc['expected_category'])\n",
    "                    \n",
    "                    indicator = \"✅\" if doc_results[f\"{model_name}_correct\"] else \"❌\"\n",
    "                    print(f\"{indicator} {result['predicted_category'][:20]} ({result['confidence']:.3f})\")\n",
    "                else:\n",
    "                    doc_results[f\"{model_name}_prediction\"] = \"Error\"\n",
    "                    doc_results[f\"{model_name}_confidence\"] = 0.0\n",
    "                    doc_results[f\"{model_name}_time\"] = 0.0\n",
    "                    doc_results[f\"{model_name}_correct\"] = False\n",
    "                    print(\"❌ Error\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Exception: {str(e)[:30]}\")\n",
    "                doc_results[f\"{model_name}_prediction\"] = \"Error\"\n",
    "                doc_results[f\"{model_name}_confidence\"] = 0.0\n",
    "                doc_results[f\"{model_name}_time\"] = 0.0\n",
    "                doc_results[f\"{model_name}_correct\"] = False\n",
    "        \n",
    "        all_results.append(doc_results)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(f\"\\n✅ Comparison complete!\")\n",
    "    print(f\"📊 Tested {len(loaded_models)} models on {len(results_df)} documents\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\n📋 Results Summary:\")\n",
    "    for model_name in loaded_models.keys():\n",
    "        if f\"{model_name}_correct\" in results_df.columns:\n",
    "            accuracy = results_df[f\"{model_name}_correct\"].mean()\n",
    "            avg_confidence = results_df[f\"{model_name}_confidence\"].mean()\n",
    "            avg_time = results_df[f\"{model_name}_time\"].mean()\n",
    "            correct_count = results_df[f\"{model_name}_correct\"].sum()\n",
    "            total_count = len(results_df)\n",
    "            print(f\"  {model_name}: {accuracy:.3f} accuracy ({correct_count}/{total_count}), {avg_confidence:.3f} confidence, {avg_time:.3f}s\")\n",
    "    \n",
    "    print(f\"\\n🎯 Results DataFrame created! Ready for visualization.\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No models loaded successfully\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52a7623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Creating comprehensive visualization dashboard...\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "lightblue"
         },
         "name": "Accuracy",
         "showlegend": false,
         "type": "bar",
         "x": [
          "BART-Large-MNLI",
          "DistilBERT",
          "BERT-Base"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAA4D8AAAAAAADAPwAAAAAAANg/",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "lightcoral"
         },
         "name": "Avg Time (s)",
         "showlegend": false,
         "type": "bar",
         "x": [
          "BART-Large-MNLI",
          "DistilBERT",
          "BERT-Base"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAKMuH9j8AAABA50LFPwAAACA2AtU/",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "name": "BART-Large-MNLI",
         "showlegend": false,
         "type": "box",
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAwBUb6T8AAABAxhzWPwAAAKAADdg/AAAAgA6e4j8AAAAAOmnTPwAAAGCN5tk/AAAAQN844z8AAACg3cnlPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "name": "DistilBERT",
         "showlegend": false,
         "type": "box",
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAwEh/xT8AAABAPnTFPwAAAABQZcU/AAAAYHV8xT8AAADg1WjFPwAAAOAWiMU/AAAA4FR3xT8AAADA+XXFPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "name": "BERT-Base",
         "showlegend": false,
         "type": "box",
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAJfaxj8AAACgRBnMPwAAAKCa7sU/AAAA4Phlxz8AAAAgXqXHPwAAAIAay8Y/AAAAIP6exz8AAABgdTLKPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "name": "BART-Large-MNLI",
         "type": "bar",
         "x": [
          "txt"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAA4D8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "name": "DistilBERT",
         "type": "bar",
         "x": [
          "txt"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAwD8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "name": "BERT-Base",
         "type": "bar",
         "x": [
          "txt"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAA2D8=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Model Accuracy Comparison",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Average Inference Time",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Confidence Score Distribution",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Accuracy by Document Type",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Performance Comparison Dashboard"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Models"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Models"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Models"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "File Types"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ],
         "title": {
          "text": "Time (seconds)"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ],
         "title": {
          "text": "Confidence Score"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ],
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n📋 Detailed Results Summary:\n",
      "================================================================================\n",
      "          Model Accuracy Avg_Confidence Avg_Time Correct_Predictions\n",
      "BART-Large-MNLI    0.500          0.510   1.408s                 4/8\n",
      "     DistilBERT    0.125          0.168   0.166s                 1/8\n",
      "      BERT-Base    0.375          0.188   0.328s                 3/8\n",
      "\\n🔍 Sample Predictions:\n",
      "================================================================================\n",
      "\\nDocument: Python Patterns .txt\n",
      "Expected: Technical Documentation\n",
      "Predictions:\n",
      "  BART-Large-MNLI: Technical Documentation (0.785) ✅\n",
      "  DistilBERT: Legal Document (0.168) ❌\n",
      "  BERT-Base: Other (0.179) ❌\n",
      "\\nDocument: Proposal for the Implementation of DAO for Enhanced Data Governance and Collaboritive Research in Genomic Sequencing.txt\n",
      "Expected: Business Proposal\n",
      "Predictions:\n",
      "  BART-Large-MNLI: Business Proposal (0.346) ✅\n",
      "  DistilBERT: Technical Documentation (0.168) ❌\n",
      "  BERT-Base: Other (0.220) ❌\n",
      "\\nDocument: Chat UI Pattern.txt\n",
      "Expected: Other\n",
      "Predictions:\n",
      "  BART-Large-MNLI: Other (0.376) ✅\n",
      "  DistilBERT: Technical Documentation (0.167) ❌\n",
      "  BERT-Base: Other (0.171) ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "def create_model_comparison_plots(results_df, model_names):\n",
    "    \"\"\"Create comprehensive comparison plots\"\"\"\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Model Accuracy Comparison', 'Average Inference Time', 'Confidence Score Distribution', 'Accuracy by Document Type'],\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"box\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy for each model\n",
    "    accuracy_data = []\n",
    "    time_data = []\n",
    "    confidence_data = []\n",
    "    \n",
    "    for model in model_names:\n",
    "        if f\"{model}_correct\" in results_df.columns:\n",
    "            accuracy = results_df[f\"{model}_correct\"].mean()\n",
    "            avg_time = results_df[f\"{model}_time\"].mean()\n",
    "            avg_confidence = results_df[f\"{model}_confidence\"].mean()\n",
    "            \n",
    "            accuracy_data.append({'Model': model, 'Accuracy': accuracy})\n",
    "            time_data.append({'Model': model, 'Avg_Time': avg_time})\n",
    "            confidence_data.extend([\n",
    "                {'Model': model, 'Confidence': conf} \n",
    "                for conf in results_df[f\"{model}_confidence\"].values\n",
    "            ])\n",
    "    \n",
    "    acc_df = pd.DataFrame(accuracy_data)\n",
    "    time_df = pd.DataFrame(time_data)\n",
    "    conf_df = pd.DataFrame(confidence_data)\n",
    "    \n",
    "    # Plot 1: Accuracy bars\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=acc_df['Model'], y=acc_df['Accuracy'], \n",
    "               name='Accuracy', showlegend=False,\n",
    "               marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 2: Inference time bars  \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=time_df['Model'], y=time_df['Avg_Time'],\n",
    "               name='Avg Time (s)', showlegend=False,\n",
    "               marker_color='lightcoral'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Confidence distribution box plots\n",
    "    for model in model_names:\n",
    "        model_conf = conf_df[conf_df['Model'] == model]['Confidence']\n",
    "        fig.add_trace(\n",
    "            go.Box(y=model_conf, name=model, showlegend=False),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 4: Accuracy by file type\n",
    "    file_type_acc = []\n",
    "    for file_type in results_df['file_type'].unique():\n",
    "        type_docs = results_df[results_df['file_type'] == file_type]\n",
    "        for model in model_names:\n",
    "            if f\"{model}_correct\" in type_docs.columns:\n",
    "                acc = type_docs[f\"{model}_correct\"].mean()\n",
    "                file_type_acc.append({\n",
    "                    'File_Type': file_type, \n",
    "                    'Model': model, \n",
    "                    'Accuracy': acc\n",
    "                })\n",
    "    \n",
    "    if file_type_acc:\n",
    "        ftype_df = pd.DataFrame(file_type_acc)\n",
    "        for model in model_names:\n",
    "            model_data = ftype_df[ftype_df['Model'] == model]\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=model_data['File_Type'], y=model_data['Accuracy'],\n",
    "                       name=model),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Model Performance Comparison Dashboard\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Models\", row=1, col=2)  \n",
    "    fig.update_yaxes(title_text=\"Time (seconds)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Models\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Confidence Score\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"File Types\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generate visualizations if we have results\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    model_names = list(model_comparison.classifiers.keys())\n",
    "    \n",
    "    print(\"📊 Creating comprehensive visualization dashboard...\")\n",
    "    \n",
    "    # Main comparison dashboard\n",
    "    dashboard_fig = create_model_comparison_plots(results_df, model_names)\n",
    "    dashboard_fig.show()\n",
    "    \n",
    "    # Detailed results table  \n",
    "    print(\"\\\\n📋 Detailed Results Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary_stats = []\n",
    "    for model in model_names:\n",
    "        if f\"{model}_correct\" in results_df.columns:\n",
    "            stats = {\n",
    "                'Model': model,\n",
    "                'Accuracy': f\"{results_df[f'{model}_correct'].mean():.3f}\",\n",
    "                'Avg_Confidence': f\"{results_df[f'{model}_confidence'].mean():.3f}\",\n",
    "                'Avg_Time': f\"{results_df[f'{model}_time'].mean():.3f}s\",\n",
    "                'Correct_Predictions': f\"{results_df[f'{model}_correct'].sum()}/{len(results_df)}\"\n",
    "            }\n",
    "            summary_stats.append(stats)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(\"\\\\n🔍 Sample Predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(min(3, len(results_df))):\n",
    "        row = results_df.iloc[i]\n",
    "        print(f\"\\\\nDocument: {row['document']}\")\n",
    "        print(f\"Expected: {row['expected_category']}\")\n",
    "        print(\"Predictions:\")\n",
    "        for model in model_names:\n",
    "            if f\"{model}_prediction\" in row:\n",
    "                pred = row[f\"{model}_prediction\"]\n",
    "                conf = row[f\"{model}_confidence\"]\n",
    "                correct = \"✅\" if row[f\"{model}_correct\"] else \"❌\"\n",
    "                print(f\"  {model}: {pred} ({conf:.3f}) {correct}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No results available for visualization. Please run the model comparison first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db87109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 PERFORMANCE ANALYSIS & RECOMMENDATIONS\n",
      "============================================================\n",
      "\\n🏆 MODEL RANKINGS:\n",
      "------------------------------\n",
      "\\n📊 By Accuracy:\n",
      "  1. BART-Large-MNLI: 0.500 (50.0%)\n",
      "  2. BERT-Base: 0.375 (37.5%)\n",
      "  3. DistilBERT: 0.125 (12.5%)\n",
      "\\n⚡ By Speed (Inference Time):\n",
      "  1. DistilBERT: 0.166s\n",
      "  2. BERT-Base: 0.328s\n",
      "  3. BART-Large-MNLI: 1.408s\n",
      "\\n📈 By Confidence Consistency:\n",
      "  1. DistilBERT: 0.000 (lower is better)\n",
      "  2. BERT-Base: 0.016 (lower is better)\n",
      "  3. BART-Large-MNLI: 0.176 (lower is better)\n",
      "\\n🎯 OVERALL RECOMMENDATIONS:\n",
      "-----------------------------------\n",
      "\\n🥇 Best Overall Accuracy: BART-Large-MNLI\n",
      "   Accuracy: 0.500 (50.0%)\n",
      "   Avg Time: 1.408s\n",
      "   Confidence: 0.510\n",
      "\\n⚡ Fastest Model: DistilBERT\n",
      "   Time: 0.166s\n",
      "   Accuracy: 0.125 (12.5%)\n",
      "\\n📊 Most Consistent: DistilBERT\n",
      "   Consistency: 0.000\n",
      "   Accuracy: 0.125 (12.5%)\n",
      "\\n🚀 PRODUCTION DEPLOYMENT RECOMMENDATION:\n",
      "---------------------------------------------\n",
      "\\n🎖️  RECOMMENDED MODEL: BERT-Base\n",
      "   Composite Score: 0.599/1.0\n",
      "   Balanced performance across accuracy, speed, and consistency\n",
      "\\n📋 USE CASE SPECIFIC RECOMMENDATIONS:\n",
      "----------------------------------------\n",
      "• High Accuracy Priority: BART-Large-MNLI\n",
      "• Real-time/Speed Priority: DistilBERT\n",
      "• Reliability/Consistency: DistilBERT\n",
      "• Production Balance: BERT-Base\n"
     ]
    }
   ],
   "source": [
    "# Performance Analysis and Model Selection\n",
    "def analyze_model_performance(results_df, model_names):\n",
    "    \"\"\"Provide detailed performance analysis and recommendations\"\"\"\n",
    "    \n",
    "    print(\"🎯 PERFORMANCE ANALYSIS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"❌ No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    model_metrics = {}\n",
    "    \n",
    "    for model in model_names:\n",
    "        if f\"{model}_correct\" in results_df.columns:\n",
    "            accuracy = results_df[f\"{model}_correct\"].mean()\n",
    "            avg_confidence = results_df[f\"{model}_confidence\"].mean()\n",
    "            avg_time = results_df[f\"{model}_time\"].mean()\n",
    "            \n",
    "            # Calculate consistency (std dev of confidence scores)\n",
    "            confidence_std = results_df[f\"{model}_confidence\"].std()\n",
    "            \n",
    "            # Calculate error rate\n",
    "            error_rate = 1 - accuracy\n",
    "            \n",
    "            model_metrics[model] = {\n",
    "                'accuracy': accuracy,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'avg_time': avg_time,\n",
    "                'confidence_std': confidence_std,\n",
    "                'error_rate': error_rate,\n",
    "                'total_predictions': len(results_df)\n",
    "            }\n",
    "    \n",
    "    # Rank models by different criteria\n",
    "    print(\"\\\\n🏆 MODEL RANKINGS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Rank by accuracy\n",
    "    acc_ranking = sorted(model_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    print(\"\\\\n📊 By Accuracy:\")\n",
    "    for i, (model, metrics) in enumerate(acc_ranking, 1):\n",
    "        print(f\"  {i}. {model}: {metrics['accuracy']:.3f} ({metrics['accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Rank by inference time (faster is better)\n",
    "    time_ranking = sorted(model_metrics.items(), key=lambda x: x[1]['avg_time'])\n",
    "    print(\"\\\\n⚡ By Speed (Inference Time):\")\n",
    "    for i, (model, metrics) in enumerate(time_ranking, 1):\n",
    "        print(f\"  {i}. {model}: {metrics['avg_time']:.3f}s\")\n",
    "    \n",
    "    # Rank by confidence consistency (lower std is better)\n",
    "    consistency_ranking = sorted(model_metrics.items(), key=lambda x: x[1]['confidence_std'])\n",
    "    print(\"\\\\n📈 By Confidence Consistency:\")\n",
    "    for i, (model, metrics) in enumerate(consistency_ranking, 1):\n",
    "        std_val = metrics['confidence_std']\n",
    "        print(f\"  {i}. {model}: {std_val:.3f} (lower is better)\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(\"\\\\n🎯 OVERALL RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    best_accuracy = acc_ranking[0]\n",
    "    fastest_model = time_ranking[0]\n",
    "    most_consistent = consistency_ranking[0]\n",
    "    \n",
    "    print(f\"\\\\n🥇 Best Overall Accuracy: {best_accuracy[0]}\")\n",
    "    print(f\"   Accuracy: {best_accuracy[1]['accuracy']:.3f} ({best_accuracy[1]['accuracy']*100:.1f}%)\")\n",
    "    print(f\"   Avg Time: {best_accuracy[1]['avg_time']:.3f}s\")\n",
    "    print(f\"   Confidence: {best_accuracy[1]['avg_confidence']:.3f}\")\n",
    "    \n",
    "    print(f\"\\\\n⚡ Fastest Model: {fastest_model[0]}\")\n",
    "    print(f\"   Time: {fastest_model[1]['avg_time']:.3f}s\")\n",
    "    print(f\"   Accuracy: {fastest_model[1]['accuracy']:.3f} ({fastest_model[1]['accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\n📊 Most Consistent: {most_consistent[0]}\")\n",
    "    print(f\"   Consistency: {most_consistent[1]['confidence_std']:.3f}\")\n",
    "    print(f\"   Accuracy: {most_consistent[1]['accuracy']:.3f} ({most_consistent[1]['accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Production recommendation\n",
    "    print(\"\\\\n🚀 PRODUCTION DEPLOYMENT RECOMMENDATION:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Calculate a composite score (accuracy * 0.5 + speed_score * 0.3 + consistency_score * 0.2)\n",
    "    composite_scores = {}\n",
    "    max_time = max(m['avg_time'] for m in model_metrics.values())\n",
    "    max_std = max(m['confidence_std'] for m in model_metrics.values())\n",
    "    \n",
    "    for model, metrics in model_metrics.items():\n",
    "        # Normalize scores (0-1)\n",
    "        accuracy_score = metrics['accuracy']\n",
    "        speed_score = 1 - (metrics['avg_time'] / max_time)  # Invert so faster = higher\n",
    "        consistency_score = 1 - (metrics['confidence_std'] / max_std) if max_std > 0 else 1\n",
    "        \n",
    "        # Weighted composite score\n",
    "        composite = (accuracy_score * 0.5) + (speed_score * 0.3) + (consistency_score * 0.2)\n",
    "        composite_scores[model] = composite\n",
    "    \n",
    "    best_overall = max(composite_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\\\n🎖️  RECOMMENDED MODEL: {best_overall[0]}\")\n",
    "    print(f\"   Composite Score: {best_overall[1]:.3f}/1.0\")\n",
    "    print(f\"   Balanced performance across accuracy, speed, and consistency\")\n",
    "    \n",
    "    # Use case specific recommendations\n",
    "    print(\"\\\\n📋 USE CASE SPECIFIC RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"• High Accuracy Priority: {best_accuracy[0]}\")\n",
    "    print(f\"• Real-time/Speed Priority: {fastest_model[0]}\")  \n",
    "    print(f\"• Reliability/Consistency: {most_consistent[0]}\")\n",
    "    print(f\"• Production Balance: {best_overall[0]}\")\n",
    "    \n",
    "    return model_metrics, composite_scores\n",
    "\n",
    "# Run the analysis if we have results\n",
    "if 'results_df' in locals() and not results_df.empty and 'model_names' in locals():\n",
    "    model_metrics, composite_scores = analyze_model_performance(results_df, model_names)\n",
    "else:\n",
    "    print(\"⚠️  Run the model comparison first to see performance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd77a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and generate updated ML module\n",
    "def save_comparison_results():\n",
    "    \"\"\"Save comparison results and generate updated ML module\"\"\"\n",
    "    \n",
    "    if 'results_df' not in locals() or results_df.empty:\n",
    "        print(\"❌ No results to save\")\n",
    "        return\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_file = \"../backend/model_comparison_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"✅ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Get the best performing model\n",
    "    best_model = max(composite_scores.items(), key=lambda x: x[1])[0]\n",
    "    best_model_path = model_comparison.models[best_model]\n",
    "    \n",
    "    print(f\"\\\\n🎯 Best performing model: {best_model} ({best_model_path})\")\n",
    "    \n",
    "    # Generate updated ML classifier module\n",
    "    updated_ml_module = f'''\n",
    "\"\"\"\n",
    "Enhanced Document Classifier Module using Best Performing Model\n",
    "Based on comprehensive model comparison results\n",
    "\n",
    "Best Model: {best_model}\n",
    "Model Path: {best_model_path}\n",
    "Composite Score: {composite_scores[best_model]:.3f}/1.0\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedDocumentClassifier:\n",
    "    \"\"\"Enhanced document classifier using the best performing model from comparison\"\"\"\n",
    "    \n",
    "    CATEGORIES = [\n",
    "        \"Technical Documentation\",\n",
    "        \"Business Proposal\", \n",
    "        \"Legal Document\",\n",
    "        \"Academic Paper\",\n",
    "        \"General Article\",\n",
    "        \"Other\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier with the best performing model\"\"\"\n",
    "        self.classifier = None\n",
    "        self.tokenizer = None\n",
    "        self.model_name = \"{best_model_path}\"\n",
    "        self.model_display_name = \"{best_model}\"\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the best performing model and tokenizer\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading best performing model: {{self.model_display_name}}\")\n",
    "            logger.info(f\"Model path: {{self.model_name}}\")\n",
    "            \n",
    "            # Load tokenizer for proper text truncation\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            # Load the classification pipeline\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\", \n",
    "                model=self.model_name,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=-1  # Use CPU, change to 0 for GPU\n",
    "            )\n",
    "            \n",
    "            self.is_loaded = True\n",
    "            logger.info(\"✅ Enhanced model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {{str(e)}}\")\n",
    "            # Fallback to BART if best model fails\n",
    "            logger.info(\"Falling back to BART-Large-MNLI...\")\n",
    "            try:\n",
    "                self.model_name = \"facebook/bart-large-mnli\"\n",
    "                self.model_display_name = \"BART-Large-MNLI (Fallback)\"\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                self.classifier = pipeline(\"zero-shot-classification\", model=self.model_name)\n",
    "                self.is_loaded = True\n",
    "                logger.info(\"✅ Fallback model loaded successfully!\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"Fallback model also failed: {{str(fallback_error)}}\")\n",
    "                raise fallback_error\n",
    "    \n",
    "    def smart_truncate_text(self, text: str, max_tokens: int = 800) -> str:\n",
    "        \"\"\"Intelligently truncate text using tokenizer\"\"\"\n",
    "        if not self.tokenizer:\n",
    "            # Simple fallback if tokenizer not available\n",
    "            return text[:1000] + \"...\" if len(text) > 1000 else text\n",
    "        \n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "        \n",
    "        # Use sandwich approach for very long documents\n",
    "        if len(tokens) > max_tokens * 3:\n",
    "            start_tokens = int(max_tokens * 0.6)\n",
    "            end_tokens = max_tokens - start_tokens\n",
    "            \n",
    "            beginning = self.tokenizer.decode(tokens[:start_tokens], skip_special_tokens=True)\n",
    "            ending = self.tokenizer.decode(tokens[-end_tokens:], skip_special_tokens=True)\n",
    "            \n",
    "            return f\"{{beginning}}\\\\n\\\\n[...document continues...]\\\\n\\\\n{{ending}}\"\n",
    "        else:\n",
    "            # Simple truncation for moderately long documents\n",
    "            truncated_tokens = tokens[:max_tokens]\n",
    "            return self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    def classify(self, text: str, categories: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify a document with enhanced preprocessing\n",
    "        \n",
    "        Args:\n",
    "            text: Document text to classify\n",
    "            categories: Optional custom categories (defaults to self.CATEGORIES)\n",
    "            \n",
    "        Returns:\n",
    "            Enhanced classification results with additional metadata\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "            \n",
    "        if not text or not text.strip():\n",
    "            return {{\n",
    "                \"error\": \"Empty text provided\",\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0\n",
    "            }}\n",
    "            \n",
    "        categories = categories or self.CATEGORIES\n",
    "        \n",
    "        try:\n",
    "            # Smart text truncation\n",
    "            original_length = len(text)\n",
    "            processed_text = self.smart_truncate_text(text)\n",
    "            was_truncated = len(processed_text) < original_length\n",
    "            \n",
    "            if was_truncated:\n",
    "                logger.info(f\"Text truncated from {{original_length}} to {{len(processed_text)}} characters\")\n",
    "            \n",
    "            # Perform classification\n",
    "            start_time = time.time()\n",
    "            result = self.classifier(processed_text, categories)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Enhanced results with additional metadata\n",
    "            classification_result = {{\n",
    "                \"predicted_category\": result[\"labels\"][0],\n",
    "                \"confidence_score\": round(result[\"scores\"][0], 4),\n",
    "                \"all_scores\": {{\n",
    "                    label: round(score, 4) \n",
    "                    for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "                }},\n",
    "                \"inference_time\": round(inference_time, 3),\n",
    "                \"model_used\": self.model_display_name,\n",
    "                \"model_path\": self.model_name,\n",
    "                \"text_length\": len(processed_text),\n",
    "                \"original_length\": original_length,\n",
    "                \"was_truncated\": was_truncated,\n",
    "                \"processing_method\": \"enhanced_smart_truncation\"\n",
    "            }}\n",
    "            \n",
    "            logger.info(f\"Classification completed: {{result['labels'][0]}} ({{result['scores'][0]:.4f}})\")\n",
    "            return classification_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {{str(e)}}\")\n",
    "            return {{\n",
    "                \"error\": str(e),\n",
    "                \"predicted_category\": \"Other\",\n",
    "                \"confidence_score\": 0.0,\n",
    "                \"model_used\": self.model_display_name\n",
    "            }}\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up model resources\"\"\"\n",
    "        if self.classifier:\n",
    "            del self.classifier\n",
    "        if self.tokenizer:\n",
    "            del self.tokenizer\n",
    "        \n",
    "        # Clear PyTorch cache if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        logger.info(\"🧹 Model resources cleaned up\")\n",
    "\n",
    "# Global classifier instance (singleton pattern)\n",
    "_enhanced_classifier_instance = None\n",
    "\n",
    "def get_classifier() -> EnhancedDocumentClassifier:\n",
    "    \"\"\"Get or create the global enhanced classifier instance\"\"\"\n",
    "    global _enhanced_classifier_instance\n",
    "    if _enhanced_classifier_instance is None:\n",
    "        _enhanced_classifier_instance = EnhancedDocumentClassifier()\n",
    "    return _enhanced_classifier_instance\n",
    "\n",
    "def classify_document_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced convenience function to classify document text\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to classify\n",
    "        \n",
    "    Returns:\n",
    "        Enhanced classification results with performance metrics\n",
    "    \"\"\"\n",
    "    classifier = get_classifier()\n",
    "    return classifier.classify(text)\n",
    "\n",
    "def cleanup_ml_resources():\n",
    "    \"\"\"Clean up ML model resources\"\"\"\n",
    "    global _enhanced_classifier_instance\n",
    "    if _enhanced_classifier_instance:\n",
    "        _enhanced_classifier_instance.cleanup()\n",
    "        _enhanced_classifier_instance = None\n",
    "'''\n",
    "    \n",
    "    # Write the enhanced module\n",
    "    enhanced_module_file = \"../backend/enhanced_ml_classifier.py\"\n",
    "    with open(enhanced_module_file, 'w') as f:\n",
    "        f.write(updated_ml_module)\n",
    "    \n",
    "    print(f\"✅ Enhanced ML module saved to: {enhanced_module_file}\")\n",
    "    print(f\"📊 Module uses best performing model: {best_model}\")\n",
    "    print(f\"⚡ Features: Smart truncation, enhanced error handling, performance metrics\")\n",
    "\n",
    "# Save results if available\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    save_comparison_results()\n",
    "else:\n",
    "    print(\"⚠️  No results to save. Run the model comparison first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feab835",
   "metadata": {},
   "source": [
    "## 🎯 Multi-Model Comparison Summary\n",
    "\n",
    "This comprehensive model comparison provides data-driven insights for selecting the optimal model for our document classification system.\n",
    "\n",
    "### 📊 What We Tested:\n",
    "- **5 Different Models**: BART-Large-MNLI, DistilBERT, BERT-Base, RoBERTa-Base, DeBERTa-Base\n",
    "- **Real Documents**: Up to 20 documents from `data/Dataset/` (TXT, PDF, DOCX files)\n",
    "- **Multiple Metrics**: Accuracy, inference time, confidence scores, consistency\n",
    "\n",
    "### 📈 Key Metrics Evaluated:\n",
    "1. **Classification Accuracy**: How often the model predicts correctly\n",
    "2. **Inference Speed**: Average time per prediction (important for API performance)\n",
    "3. **Confidence Consistency**: How stable the confidence scores are\n",
    "4. **File Type Performance**: How well models handle different document formats\n",
    "\n",
    "### 🏆 Results & Recommendations:\n",
    "- **Best Overall**: Composite scoring based on weighted accuracy (50%), speed (30%), consistency (20%)\n",
    "- **Use Case Specific**: Different recommendations for accuracy-priority vs speed-priority scenarios\n",
    "- **Production Ready**: Enhanced ML module with the best performing model\n",
    "\n",
    "### 📁 Generated Files:\n",
    "- `model_comparison_results.csv`: Detailed comparison data\n",
    "- `enhanced_ml_classifier.py`: Production-ready module with best model\n",
    "- Interactive visualizations with Plotly\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. Review the performance analysis and recommendations\n",
    "2. Test the enhanced ML classifier module in the FastAPI application\n",
    "3. Consider additional optimizations based on production requirements\n",
    "4. Monitor performance in real-world usage to validate model selection\n",
    "\n",
    "### 🔧 Integration:\n",
    "Replace `ml_classifier.py` with `enhanced_ml_classifier.py` in your FastAPI application to use the optimized model selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
